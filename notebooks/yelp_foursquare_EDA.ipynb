{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:23:07.477745Z",
     "start_time": "2024-11-17T21:23:07.475111Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import ast"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## City Bikes Data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:23:07.494435Z",
     "start_time": "2024-11-17T21:23:07.490984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get all networks from CSV file\n",
    "city_bikes = pd.read_csv('../data/city_bikes.csv')\n",
    "# Print the coordinates for the networks\n",
    "for i, station in city_bikes.head(1).iterrows():\n",
    "    print('latitude :', station['latitude'], 'longitude :', station['longitude'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latitude : 53.033019 longitude : 18.599727\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:23:07.574868Z",
     "start_time": "2024-11-17T21:23:07.570815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load environment\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foursquare"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:23:07.596322Z",
     "start_time": "2024-11-17T21:23:07.594349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fsq_api_key = os.getenv('FOURSQUARE_API_KEY')\n",
    "# Set FourSquare URL\n",
    "fsq_url = 'https://api.foursquare.com/v3/places/search'\n",
    "# Create dictionary for headers and add API KEY\n",
    "fsq_headers = {'Accept': 'application/json', 'Authorization': fsq_api_key}\n"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Send a request to Foursquare with a small radius (1000m) for all the bike stations in your city of choice. "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:23:07.610071Z",
     "start_time": "2024-11-17T21:23:07.608039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Getting parameters for the api query string\n",
    "def get_fsq_params(lat, lon):\n",
    "    return {\n",
    "        'll': f'{lat},{lon}',\n",
    "        'radius': 1000,\n",
    "        'categories': ','.join([\n",
    "            '4d4b7105d754a06374d81259',  # Restaurant,\n",
    "            '4bf58dd8d48988d116941735',  # Bar,\n",
    "            '4bf58dd8d48988d1fa931735'  # Hotel\n",
    "        ]),\n",
    "        # 'limit': 10 # max 50\n",
    "        'fields': 'name,categories,distance,rating,price'\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:23:07.620032Z",
     "start_time": "2024-11-17T21:23:07.618184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Send API requests\n",
    "def get_fsq_response(url, headers, params):\n",
    "    return requests.get(url, headers=headers, params=params)"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Parse through the response to get the POI (such as restaurants, bars, etc) details you want (ratings, name, location, etc)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:24:03.857722Z",
     "start_time": "2024-11-17T21:23:42.933523Z"
    }
   },
   "source": [
    "#FourSquare DataFrame\n",
    "fsq_df = pd.DataFrame()\n",
    "# Loop once to save on api usage\n",
    "for i, station in city_bikes.head(1).iterrows():\n",
    "    # Set params\n",
    "    latitude, longitude = station['latitude'], station['longitude']\n",
    "    fsq_params = get_fsq_params(latitude, longitude)\n",
    "    # Get response\n",
    "    fsq_response = get_fsq_response(fsq_url, fsq_headers, fsq_params)\n",
    "    # Normalize response JSON to fit DataFrame\n",
    "    df = pd.json_normalize(\n",
    "        fsq_response.json(), record_path='results'\n",
    "    )\n",
    "    fsq_df = pd.concat([fsq_df, df], ignore_index=True)\n",
    "# Write the data to CSV - IDEALLY SHOULD APPEND  \n",
    "fsq_df.to_csv('../data/fsq_search_data.csv', sep=',', index=False)"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Put your parsed results into a DataFrame"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:32:37.970328Z",
     "start_time": "2024-11-17T21:32:37.963959Z"
    }
   },
   "source": [
    "fsq_df = pd.read_csv('../data/fsq_search_data.csv')\n",
    "# fsq_df.head()"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:23:08.269846Z",
     "start_time": "2024-11-17T21:23:08.267813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get Yelp API Key\n",
    "yelp_api_key = os.environ['YELP_API_KEY']\n",
    "# Set Yelp URL\n",
    "yelp_url = 'https://api.yelp.com/v3/businesses/search'\n",
    "# Create dictionary for headers and add API KEY\n",
    "yelp_headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Authorization': f'Bearer {yelp_api_key}'\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Send a request to Yelp with a small radius (1000m) for all the bike stations in your city of choice. "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:23:08.280573Z",
     "start_time": "2024-11-17T21:23:08.278835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Getting parameters for the api query string\n",
    "def get_yelp_params(lat, lon):\n",
    "    return {\n",
    "        'latitude': lat,\n",
    "        'longitude': lon,\n",
    "        'radius': 1000,\n",
    "        'limit': 10,\n",
    "        'categories': 'Bars,Restaurants,Hotels'\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:23:08.290857Z",
     "start_time": "2024-11-17T21:23:08.289372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Send API requests\n",
    "def get_yelp_response(url, headers, params):\n",
    "    return requests.get(url, headers=headers, params=params)"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Parse through the response to get the POI (such as restaurants, bars, etc) details you want (ratings, name, location, etc)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:23:08.525552Z",
     "start_time": "2024-11-17T21:23:08.298857Z"
    }
   },
   "source": [
    "#Yelp DataFrame\n",
    "yelp_df = pd.DataFrame()\n",
    "\n",
    "for i, station in city_bikes.head(1).iterrows():\n",
    "    # Set params\n",
    "    latitude, longitude = station['latitude'], station['longitude']\n",
    "    yelp_params = get_yelp_params(latitude, longitude)\n",
    "    # Get response\n",
    "    yelp_response = get_yelp_response(yelp_url, yelp_headers, yelp_params)\n",
    "    # Normalize response JSON to fit DataFrame\n",
    "    df = pd.json_normalize(\n",
    "        yelp_response.json(), record_path='businesses'\n",
    "    )\n",
    "    yelp_df = pd.concat([yelp_df, df], ignore_index=True)\n",
    "\n",
    "# Write the data to CSV - IDEALLY SHOULD APPEND\n",
    "yelp_df.to_csv('../data/yelp_search_data.csv', sep=',', index=False)"
   ],
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'businesses' not found. If specifying a record_path, all elements of data should have the path.\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/LHL-BootCamp/LHL-Final-Project-Statistical-Modelling-with-Python/Final Project Statistical Modeling/venv/lib/python3.13/site-packages/pandas/io/json/_normalize.py:398\u001B[0m, in \u001B[0;36mjson_normalize.<locals>._pull_field\u001B[0;34m(js, spec, extract_record)\u001B[0m\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 398\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mresult\u001B[49m\u001B[43m[\u001B[49m\u001B[43mspec\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mKeyError\u001B[0m: 'businesses'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[57], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m     yelp_response \u001B[38;5;241m=\u001B[39m get_yelp_response(yelp_url, yelp_headers, yelp_params)\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m# Normalize response JSON to fit DataFrame\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjson_normalize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43myelp_response\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecord_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbusinesses\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     yelp_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([yelp_df, df], ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Write the data to CSV - IDEALLY SHOULD APPEND\u001B[39;00m\n",
      "File \u001B[0;32m~/LHL-BootCamp/LHL-Final-Project-Statistical-Modelling-with-Python/Final Project Statistical Modeling/venv/lib/python3.13/site-packages/pandas/io/json/_normalize.py:517\u001B[0m, in \u001B[0;36mjson_normalize\u001B[0;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001B[0m\n\u001B[1;32m    514\u001B[0m                 meta_vals[key]\u001B[38;5;241m.\u001B[39mappend(meta_val)\n\u001B[1;32m    515\u001B[0m             records\u001B[38;5;241m.\u001B[39mextend(recs)\n\u001B[0;32m--> 517\u001B[0m \u001B[43m_recursive_extract\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecord_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    519\u001B[0m result \u001B[38;5;241m=\u001B[39m DataFrame(records)\n\u001B[1;32m    521\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m record_prefix \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/LHL-BootCamp/LHL-Final-Project-Statistical-Modelling-with-Python/Final Project Statistical Modeling/venv/lib/python3.13/site-packages/pandas/io/json/_normalize.py:499\u001B[0m, in \u001B[0;36mjson_normalize.<locals>._recursive_extract\u001B[0;34m(data, path, seen_meta, level)\u001B[0m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    498\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m data:\n\u001B[0;32m--> 499\u001B[0m         recs \u001B[38;5;241m=\u001B[39m \u001B[43m_pull_records\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    500\u001B[0m         recs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    501\u001B[0m             nested_to_record(r, sep\u001B[38;5;241m=\u001B[39msep, max_level\u001B[38;5;241m=\u001B[39mmax_level)\n\u001B[1;32m    502\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(r, \u001B[38;5;28mdict\u001B[39m)\n\u001B[1;32m    503\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m r\n\u001B[1;32m    504\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m recs\n\u001B[1;32m    505\u001B[0m         ]\n\u001B[1;32m    507\u001B[0m         \u001B[38;5;66;03m# For repeating the metadata later\u001B[39;00m\n",
      "File \u001B[0;32m~/LHL-BootCamp/LHL-Final-Project-Statistical-Modelling-with-Python/Final Project Statistical Modeling/venv/lib/python3.13/site-packages/pandas/io/json/_normalize.py:421\u001B[0m, in \u001B[0;36mjson_normalize.<locals>._pull_records\u001B[0;34m(js, spec)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_pull_records\u001B[39m(js: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any], spec: \u001B[38;5;28mlist\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m    416\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;124;03m    Internal function to pull field for records, and similar to\u001B[39;00m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;124;03m    _pull_field, but require to return list. And will raise error\u001B[39;00m\n\u001B[1;32m    419\u001B[0m \u001B[38;5;124;03m    if has non iterable value.\u001B[39;00m\n\u001B[1;32m    420\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 421\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_pull_field\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspec\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextract_record\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    423\u001B[0m     \u001B[38;5;66;03m# GH 31507 GH 30145, GH 26284 if result is not list, raise TypeError if not\u001B[39;00m\n\u001B[1;32m    424\u001B[0m     \u001B[38;5;66;03m# null, otherwise return an empty list\u001B[39;00m\n\u001B[1;32m    425\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, \u001B[38;5;28mlist\u001B[39m):\n",
      "File \u001B[0;32m~/LHL-BootCamp/LHL-Final-Project-Statistical-Modelling-with-Python/Final Project Statistical Modeling/venv/lib/python3.13/site-packages/pandas/io/json/_normalize.py:401\u001B[0m, in \u001B[0;36mjson_normalize.<locals>._pull_field\u001B[0;34m(js, spec, extract_record)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m extract_record:\n\u001B[0;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\n\u001B[1;32m    402\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKey \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found. If specifying a record_path, all elements of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    403\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata should have the path.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    404\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    405\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    406\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnan\n",
      "\u001B[0;31mKeyError\u001B[0m: \"Key 'businesses' not found. If specifying a record_path, all elements of data should have the path.\""
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Put your parsed results into a DataFrame"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FourSquare Data "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:33:16.088850Z",
     "start_time": "2024-11-17T21:33:16.022760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Read from CSV\n",
    "fsq_df = pd.read_csv('../data/fsq_search_data.csv')\n",
    "# Safely evaluate the string as a Python dictionary\n",
    "fsq_df['categories'] = fsq_df['categories'].apply(ast.literal_eval)\n",
    "# Explode the data by categories\n",
    "fsq_df = fsq_df.explode('categories')\n",
    "# Explode the dictionary\n",
    "categories = fsq_df['categories'].apply(pd.Series)\n",
    "# Rename the categories columns\n",
    "categories.columns = ['category.id', 'category.short', 'category.name', 'category.plural', 'category.img_url']\n",
    "# categories.head()\n",
    "# Replace categories column by columns\n",
    "fsq_df = pd.concat([fsq_df.drop(columns=['categories']), categories], axis=1)\n",
    "print(fsq_df.head(1))\n",
    "# Choose columns for EDA\n",
    "fsq_df = fsq_df[['name','price', 'rating', 'distance', 'category.plural', 'category.name']]\n",
    "# fsq_df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   distance        name  price  rating  category.id category.short  \\\n",
      "0       670  Montenegro    1.0     8.0        13064       Pizzeria   \n",
      "\n",
      "  category.name category.plural  \\\n",
      "0         Pizza       Pizzerias   \n",
      "\n",
      "                                    category.img_url  \n",
      "0  {'prefix': 'https://ss3.4sqi.net/img/categorie...  \n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## YELP Data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:56:23.420576Z",
     "start_time": "2024-11-17T21:56:23.405879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Read from CSV\n",
    "yelp_df = pd.read_csv('../data/yelp_search_data.csv')\n",
    "# Safely evaluate the string as a Python dictionary\n",
    "yelp_df['categories'] = yelp_df['categories'].apply(ast.literal_eval)\n",
    "# Explode the data by categories\n",
    "yelp_df = yelp_df.explode('categories')\n",
    "# Explode the dictionary\n",
    "categories = yelp_df['categories'].apply(pd.Series)\n",
    "# Rename the categories columns\n",
    "categories.columns = ['category.alias', 'category.title']\n",
    "# Replace categories column by columns\n",
    "yelp_df = pd.concat([yelp_df.drop(columns=['categories']), categories], axis=1)\n",
    "# Choose columns for EDA\n",
    "yelp_df = yelp_df[['name', 'review_count', 'rating', 'price', 'distance', 'business_hours', 'category.title']]\n",
    "yelp_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                name  review_count  rating price     distance  \\\n",
       "0    Pub Zielona Gęś            14     3.3    $$   123.908924   \n",
       "1  Ministerstwo Kawy            51     4.2     $  1109.084599   \n",
       "1  Ministerstwo Kawy            51     4.2     $  1109.084599   \n",
       "2       Pardon To Tu            28     4.4    $$   850.986161   \n",
       "2       Pardon To Tu            28     4.4    $$   850.986161   \n",
       "\n",
       "                                      business_hours category.title  \n",
       "0  [{'open': [{'is_overnight': True, 'start': '09...    Sports Bars  \n",
       "1  [{'open': [{'is_overnight': False, 'start': '0...          Cafes  \n",
       "1  [{'open': [{'is_overnight': False, 'start': '0...   Coffee & Tea  \n",
       "2  [{'open': [{'is_overnight': False, 'start': '0...  Vinyl Records  \n",
       "2  [{'open': [{'is_overnight': False, 'start': '0...     Bookstores  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>rating</th>\n",
       "      <th>price</th>\n",
       "      <th>distance</th>\n",
       "      <th>business_hours</th>\n",
       "      <th>category.title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pub Zielona Gęś</td>\n",
       "      <td>14</td>\n",
       "      <td>3.3</td>\n",
       "      <td>$$</td>\n",
       "      <td>123.908924</td>\n",
       "      <td>[{'open': [{'is_overnight': True, 'start': '09...</td>\n",
       "      <td>Sports Bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ministerstwo Kawy</td>\n",
       "      <td>51</td>\n",
       "      <td>4.2</td>\n",
       "      <td>$</td>\n",
       "      <td>1109.084599</td>\n",
       "      <td>[{'open': [{'is_overnight': False, 'start': '0...</td>\n",
       "      <td>Cafes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ministerstwo Kawy</td>\n",
       "      <td>51</td>\n",
       "      <td>4.2</td>\n",
       "      <td>$</td>\n",
       "      <td>1109.084599</td>\n",
       "      <td>[{'open': [{'is_overnight': False, 'start': '0...</td>\n",
       "      <td>Coffee &amp; Tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pardon To Tu</td>\n",
       "      <td>28</td>\n",
       "      <td>4.4</td>\n",
       "      <td>$$</td>\n",
       "      <td>850.986161</td>\n",
       "      <td>[{'open': [{'is_overnight': False, 'start': '0...</td>\n",
       "      <td>Vinyl Records</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pardon To Tu</td>\n",
       "      <td>28</td>\n",
       "      <td>4.4</td>\n",
       "      <td>$$</td>\n",
       "      <td>850.986161</td>\n",
       "      <td>[{'open': [{'is_overnight': False, 'start': '0...</td>\n",
       "      <td>Bookstores</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Which API provided you with more complete data? Provide an explanation? "
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Each APIs have different approach to handle the data. The outcome of a query depends on various parameters. I wish I could test all the possible options"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Get the top 10 restaurants according to their rating"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FourSquare Top 10 by ranking "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:56:29.356009Z",
     "start_time": "2024-11-17T21:56:29.350205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove duplicate rows\n",
    "df = fsq_df.drop_duplicates().copy()\n",
    "# print(df.head())\n",
    "# Sort data by rating\n",
    "df.sort_values(by='rating', ascending=False, inplace=True)\n",
    "# Display top 10\n",
    "df.head(10)\n",
    "# Write the data to CSV - for EDA\n",
    "df.to_csv('../data/fsq_for_EDA.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### YELP Top 10 by ranking"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:56:32.432129Z",
     "start_time": "2024-11-17T21:56:32.427324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove duplicate rows\n",
    "df = yelp_df.drop_duplicates().copy()\n",
    "# Sort data by rating\n",
    "df.sort_values(by='rating', ascending=False, inplace=True)\n",
    "# Display top 10\n",
    "df.head(10)\n",
    "# Write the data to CSV - for EDA\n",
    "df.to_csv('../data/yelp_for_EDA.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing Area"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:23:08.531830Z",
     "start_time": "2024-11-17T18:29:03.192035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Open and read the JSON file\n",
    "# with open('fsq_search_response.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "# data = pd.read_json('fsq_search_columns.json')\n",
    "# data = data.explode('categories').reset_index(drop=True)\n",
    "# data['categories'] = data['categories'].apply(lambda x: x['short_name'])\n",
    "# data.drop([\n",
    "#     'geocodes.drop_off.latitude',\n",
    "#     'geocodes.drop_off.longitude',\n",
    "#     'geocodes.roof.latitude',\n",
    "#     'geocodes.roof.longitude',\n",
    "# ], axis=1, inplace=True)\n",
    "# data"
   ],
   "outputs": [],
   "execution_count": 474
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
